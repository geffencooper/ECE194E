{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set up random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "seed = 23\n",
    "rng = default_rng(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generator Class\n",
    "* All the code is in this class\n",
    "* Everything is explained in the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Overview:\n",
    "        This Class contains all the functions needed for creating a markov chain from a corpus of\n",
    "        text and generating new text from the created markov chain. It also adds an option for \n",
    "        increasing the history to be greater than one so that we consider multiple words when\n",
    "        generating a new word. While this does break the markov property, it can create more\n",
    "        realistic text and was an interesting problem to explore.\n",
    "\n",
    "Usage:  The markov chain must be created from a corpus of text which will be passed\n",
    "        in through a .txt file. The user can also specify the desired history size to\n",
    "        consider when generating text. Once the TextGenerator object is created, we must\n",
    "        first parse the text into tokens using the text_to_list() function. Then we can pass\n",
    "        these tokens into the gen_word_dist() function to create the markov chain which is\n",
    "        just a nested dictionary. We don't create a matrix since most of the elements will\n",
    "        be zero anyways and this allows for a more intuitive representation. This step can\n",
    "        be thought of as 'training', we are filling in all the state transition probabilities.\n",
    "        Finally, we call the generate_text() function which steps through the markov chain\n",
    "        using the get_next_state() function. This will output the final generated text in a \n",
    "        readable (might be gibberish) form.\n",
    "'''\n",
    "\n",
    "class TextGenerator:\n",
    "    '''\n",
    "    Description: This function is the constructor.\n",
    "\n",
    "    Input: txt_file_path - Relative path to the txt file to generate the markov chain\n",
    "           history_size - the number of tokens per element (only history of size 1 is valid markov chain).\n",
    "    '''\n",
    "    def __init__(self,txt_file_path,history_size):\n",
    "        self.txt_file_path = txt_file_path\n",
    "        self.history_size = history_size\n",
    "\n",
    "        # list of each text token including punctuation in order they appear\n",
    "        self.ordered_token_list = []\n",
    "\n",
    "        # list of each text sequence (for history > 1) in order they appear\n",
    "        self.ordered_sequence_list = []\n",
    "\n",
    "\n",
    "    '''\n",
    "    Description: This function reads thr .txt file\n",
    "\n",
    "    Input: none\n",
    "\n",
    "    Returns: - text from the .txt file as a string\n",
    "    '''\n",
    "    def get_text(self):\n",
    "        with open(self.txt_file_path,encoding='utf-8') as f:\n",
    "            self.contents = f.read()\n",
    "        return self.contents\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Description: This function takes a string of text and turns it into a list where\n",
    "                 each element represents  a single text 'token' including punctuation\n",
    "                 (These are the states of the markov chain). If desired, we can also\n",
    "                 split the text into multitoken elements so that we can consider a\n",
    "                 history of tokens (this breaks the markov property but is interesting\n",
    "                 to explore). \n",
    "\n",
    "    Input: text - A string of text\n",
    "\n",
    "    Returns: - A list of individual tokens elements in chronological order,\n",
    "             - A list of all the multitoken elements in chronological order\n",
    "               (None if history size is 1).\n",
    "    '''\n",
    "    def text_to_list(self,text):\n",
    "        # insert spaces so we can use the split function while keeping punctuation\n",
    "        text = text.replace(\". \",\" . \").replace(\"! \",\" ! \").replace(\"? \",\" ? \").replace(\", \",\" , \")\\\n",
    "                .replace(\"; \",\" ; \").replace(\": \",\" : \").replace(\"\\\"\",\"\").replace(\".\\n\",\" . <NL> \")\\\n",
    "                .replace(\",\\n\",\" , <NL> \").replace(\"!\\n\",\" ! <NL> \").replace(\"?\\n\",\" ? <NL> \")\\\n",
    "                .replace(\":\\n\",\" : <NL> \")\n",
    "\n",
    "        # replace special characters with tokens\n",
    "        text = text.replace(\"\\t\", \" <TAB>\").replace(\"\\n\", \" <NL> \")\n",
    "\n",
    "        # now split the text using a space as the delimeter\n",
    "        self.ordered_token_list = text.split()\n",
    "\n",
    "        # if each token is a state then we are done\n",
    "        if self.history_size == 1:\n",
    "            return self.ordered_token_list, None\n",
    "\n",
    "        # for multitoken sequences, combine items in the list\n",
    "        idx = 0\n",
    "        while idx < len(self.ordered_token_list):\n",
    "            # Don't go over the list length\n",
    "            if idx + self.history_size-1 >= len(self.ordered_token_list):\n",
    "                break\n",
    "\n",
    "            # combine items based on history size\n",
    "            self.ordered_sequence_list.append(' '.join(self.ordered_token_list[idx : idx + self.history_size]))\n",
    "            idx += 1\n",
    "\n",
    "        return self.ordered_token_list, self.ordered_sequence_list \n",
    "\n",
    "\n",
    "    '''\n",
    "    Description: This function takes the ordered text list and creates\n",
    "                 a markov chain representation from it. Each token represents a state \n",
    "                 and the token immediately following represents a potential\n",
    "                 next state. We can represent this as a nested dictionary where at\n",
    "                 the first level we have all the unique words in the corpus (i.e. the states)\n",
    "                 and at the second level we have all the potential next states and\n",
    "                 their relative probability of being transitioned to.\n",
    "                \n",
    "                 When the history size is greater than one, we will use multiple words to \n",
    "                 determine the next word. This technically breaks the markov property but\n",
    "                 will enable more realistic text by considering the history of words instead\n",
    "                 of only the current one since in sentences each word correlates with more\n",
    "                 than just the word before it.\n",
    "\n",
    "    Input: token_list - An ordered list of individual text tokens.\n",
    "           multitoken_list - List of multitoken elements for history_size > 1\n",
    "           all_states - Bool of whether to include all possible outgoing states (even with prob = 0)\n",
    "\n",
    "    Returns: A nested dictionary representing the Markov Chain. In the case where the\n",
    "             history size is greater than 1, the first level will be all the unique\n",
    "             multitoken elements and the second level will be the possible next individual words.\n",
    "    '''\n",
    "    def gen_word_dist(self, token_list, multitoken_list=None,all_states=False):\n",
    "        # save in case we want to visualize the stat graph\n",
    "        self.all_states = all_states\n",
    "\n",
    "        # case when only consider the current token, markov property is true\n",
    "        if self.history_size == 1:\n",
    "            # create the first level from the unique tokens (all the states)\n",
    "            unique_tokens = set(token_list)\n",
    "            self.text_dict = dict.fromkeys(unique_tokens)\n",
    "\n",
    "            # create a nested dictionary for each unique token (all the outgoing states)\n",
    "            for token in self.text_dict.keys():\n",
    "                if all_states:\n",
    "                    self.text_dict[token] = dict.fromkeys(unique_tokens,0)\n",
    "                else:\n",
    "                    self.text_dict[token] = {}\n",
    "            \n",
    "            # now add the words that follow each unique token\n",
    "            # where the key is the following word and the value is the count\n",
    "            for idx,token in enumerate(token_list[1:]):\n",
    "                try: # try to increment the count of the token\n",
    "                    self.text_dict[token_list[idx]][token] += 1\n",
    "                except KeyError: # otherwise set it as the first occurence\n",
    "                    self.text_dict[token_list[idx]][token] = 1\n",
    "\n",
    "            # now we convert the counts to probabilities\n",
    "            for state in self.text_dict.keys():\n",
    "                total = sum(self.text_dict[state].values())\n",
    "                for out_state in self.text_dict[state].keys():\n",
    "                    self.text_dict[state][out_state] = self.text_dict[state][out_state]/total\n",
    "                    \n",
    "            return self.text_dict\n",
    "        \n",
    "        # case when past tokens are considered, markov property is broken\n",
    "        else:\n",
    "            # create the first level from the unique tokens (all the states)\n",
    "            unique_sequences = set(multitoken_list)\n",
    "            self.text_dict = dict.fromkeys(unique_sequences)\n",
    "\n",
    "            # create a nested dictionary for each unique token (all the outgoing states)\n",
    "            for sequence in self.text_dict.keys():\n",
    "                self.text_dict[sequence] = {}\n",
    "            \n",
    "            # now add the words that follow each unique sequence\n",
    "            # where the key is the following word and the value is the count\n",
    "            token_idx = self.history_size\n",
    "            for idx,sequence in enumerate(multitoken_list[:-1]):\n",
    "                try: # try to increment the count of the token\n",
    "                    self.text_dict[multitoken_list[idx]][token_list[token_idx]] += 1\n",
    "                except KeyError: # otherwise set it as the first occurence\n",
    "                    self.text_dict[multitoken_list[idx]][token_list[token_idx]] = 1\n",
    "                token_idx += 1\n",
    "\n",
    "            # now we convert the counts to probabilities\n",
    "            for state in self.text_dict.keys():\n",
    "                total = sum(self.text_dict[state].values())\n",
    "                for out_state in self.text_dict[state].keys():\n",
    "                    self.text_dict[state][out_state] = self.text_dict[state][out_state]/total\n",
    "                    \n",
    "            return self.text_dict\n",
    "\n",
    "\n",
    "    '''\n",
    "    Description: This function will take a given state in the markov chain and\n",
    "                 select the next state probabilistically.\n",
    "\n",
    "    Input: The current state as a dictionary.\n",
    "\n",
    "    Returns: The next state.\n",
    "    '''\n",
    "    def get_next_state(self,current_state):\n",
    "        # get next states that branch from current one, and their relative probabilities\n",
    "        out_states = list(current_state.keys())\n",
    "        probs = list(current_state.values())\n",
    "        \n",
    "        # make sure probabilities sum to 1\n",
    "        probs[0] += 1-sum(probs)\n",
    "\n",
    "        # choose a state randomly based on probabilities\n",
    "        return rng.choice(a=out_states,size=1,p=probs)\n",
    "\n",
    "\n",
    "    '''\n",
    "    Description: This function will step through the generated markov chain\n",
    "                 to produce sentences based on the input parameters. It will\n",
    "                 format the text as well.\n",
    "\n",
    "    Input: The first word (must be in the corpus), the number of desired sentences,\n",
    "           the markov chain dictionary\n",
    "\n",
    "    Returns: The formatted output text.\n",
    "    '''\n",
    "    def generate_text(self,first_word,num_sentences,markov_chain):\n",
    "        if self.history_size == 1:\n",
    "            # get the state from the desired first word\n",
    "            curr_state = markov_chain[first_word]\n",
    "\n",
    "            # start the text sequence\n",
    "            text = [first_word]\n",
    "\n",
    "            sentence_count = 0\n",
    "            # keep adding words until we reach the sentence count\n",
    "            while sentence_count < num_sentences:\n",
    "                # get the next word\n",
    "                next_state = self.get_next_state(curr_state)\n",
    "\n",
    "                # add this word to the text sequence (only get the string)\n",
    "                text.append(next_state.tolist()[0])\n",
    "\n",
    "                # set the current state to the next state (only get the string)\n",
    "                curr_state = markov_chain[next_state[0]]\n",
    "\n",
    "                # check if the sentence ended\n",
    "                if next_state[0] == '.' or next_state[0] == '?' or next_state[0] == '!':\n",
    "                    sentence_count += 1\n",
    "\n",
    "            # format the text sequence into a sentence\n",
    "            text_string = \" \".join(text)\n",
    "            text_string = text_string.replace(\" . \",\". \").replace(\" , \",\", \").replace(\" ; \",\"; \").replace(\" ! \",\"! \").replace(\" ? \",\"? \").replace(\" : \",\": \")\n",
    "            text_string = text_string.replace(\"<TAB> \",\"\\t\").replace(\"<NL> \",\"\\n\")\n",
    "            text_string_final = text_string[:-2]+text_string[-1]\n",
    "            return text_string_final\n",
    "        else:\n",
    "            # get the state from the desired first word\n",
    "            curr_state = markov_chain[first_word]\n",
    "\n",
    "            # start the text sequence\n",
    "            text = first_word.split()\n",
    "\n",
    "            sentence_count = 0\n",
    "            token_idx = self.history_size\n",
    "            # keep adding words until we reach the sentence count\n",
    "            while sentence_count < num_sentences:\n",
    "                # get the next word\n",
    "                next_word = self.get_next_state(curr_state)\n",
    "\n",
    "                # add this word to the text sequence (only get the string)\n",
    "                text.append(next_word.tolist()[0])\n",
    "\n",
    "                # sometimes get weird case where key doesn't exist\n",
    "                try:\n",
    "                    curr_state = markov_chain[\" \".join(text[token_idx-self.history_size+1:token_idx+1])]\n",
    "\n",
    "                except KeyError:\n",
    "                    print(\"==== Exception ====\")\n",
    "                    print(token_idx,text)\n",
    "                    print(\"\\n\\n\")\n",
    "                    print(text[token_idx-self.history_size+1])\n",
    "                    print(\"\\n\\n\")\n",
    "                    print(text[token_idx-self.history_size+1:token_idx+1])\n",
    "                    break\n",
    "                \n",
    "                # check if there is no next state\n",
    "                if curr_state == {}:\n",
    "                    curr_state = markov_chain[first_word]\n",
    "\n",
    "                # check if the sentence ended\n",
    "                if next_word[0] == '.' or next_word[0] == '?' or next_word[0] == '!':\n",
    "                    sentence_count += 1\n",
    "                \n",
    "                token_idx += 1\n",
    "\n",
    "            # format the text sequence into a sentence\n",
    "            text_string = \" \".join(text)\n",
    "            text_string = text_string.replace(\" . \",\". \").replace(\" , \",\", \").replace(\" ; \",\"; \").replace(\" ! \",\"! \").replace(\" ? \",\"? \").replace(\" : \",\": \")\n",
    "            text_string = text_string.replace(\"<TAB> \",\"\\t\").replace(\"<NL> \",\"\\n\")\n",
    "            text_string_final = text_string[:-2]+text_string[-1]\n",
    "            return text_string_final\n",
    "\n",
    "\n",
    "    '''\n",
    "    Description: This function will generate a state transition plot from the dictionary.\n",
    "                 This will only work for a history size of 1 since this is the only\n",
    "                 valid markov chain.\n",
    "\n",
    "    Input: state_dict - output of gen_word_dist()\n",
    "\n",
    "    Returns: State Transition Matrix, state_labels\n",
    "    '''\n",
    "    def generate_transition_graph(self,state_dict):\n",
    "        # regenrate graph with all states\n",
    "        if self.all_states == False:\n",
    "            state_dict = self.gen_word_dist(self.ordered_token_list,None,True)\n",
    "        \n",
    "        # generate transition matrix\n",
    "        states = list(state_dict.keys())\n",
    "        dim = len(states)\n",
    "        state_transition_matrix = np.zeros((dim,dim))\n",
    "        for row in range(dim):\n",
    "            state_transition_matrix[row,:] = np.fromiter(state_dict[states[row]].values(), dtype=float)\n",
    "\n",
    "        return state_transition_matrix, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the functionality of each function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the text_to_list() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['My', 'name', 'is', 'Geffen', 'Cooper', '.', 'What', 'is', 'yours', '?'],\n",
       " None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dummy object for testing\n",
    "history = 1\n",
    "gen = TextGenerator(\"\",history)\n",
    "\n",
    "# sample corpus\n",
    "text = \"My name is Geffen Cooper. What is yours? \"\n",
    "\n",
    "# show the list output\n",
    "gen.text_to_list(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the gen_word_dist() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'very': {'tall': 0.5, 'happy': 0.5},\n",
       " '.': {'He': 1.0},\n",
       " 'He': {'is': 1.0},\n",
       " 'tall': {'.': 1.0},\n",
       " 'is': {'very': 1.0},\n",
       " 'happy': {'.': 1.0}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dummy object for testing\n",
    "history = 1\n",
    "gen = TextGenerator(\"\",history)\n",
    "\n",
    "# sample corpus\n",
    "text = \"He is very tall. He is very happy. \"\n",
    "tl, ml = gen.text_to_list(text)\n",
    "\n",
    "gen.gen_word_dist(tl,ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi all, \n",
      "\n",
      "---- \n",
      "Hi all, \n",
      "\n",
      "Hope that you can now submit it by Tuesday 11:59 PM. As you know, there can be at the level of labs 2-4 so basically design a new lab with report; 2) read and study a related machine learning research paper and write a summary in a report or survey paper discussing the algorithm, results, strengths and weaknesses.\n"
     ]
    }
   ],
   "source": [
    "# --- parameters ---\n",
    "history = 2\n",
    "file = \"1789-04-30-first-inaugural-address.txt\"\n",
    "file = \"email.txt\"\n",
    "first_word = \"Hi all\"\n",
    "num_sentences = 2\n",
    "\n",
    "# create the generator from a certain corpus\n",
    "gen = TextGenerator(file,history)\n",
    "\n",
    "# get the raw text\n",
    "raw_text = gen.get_text()\n",
    "\n",
    "# parse the text\n",
    "token_list, sequence_list = gen.text_to_list(raw_text)\n",
    "\n",
    "# generate the markov chain\n",
    "dist = gen.gen_word_dist(token_list,sequence_list)\n",
    "\n",
    "# generate the text\n",
    "print(gen.generate_text(first_word,num_sentences,dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_array(arr):\n",
    "    \"\"\"\n",
    "    prints a 2-D numpy array in a nicer format\n",
    "    \"\"\"\n",
    "    for a in arr:\n",
    "        for elem in a:\n",
    "            print(\"{} \".format(elem).rjust(3), end=\"\")\n",
    "        print(end=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 \n",
      "1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 \n",
      "0.0 0.0 0.3333333333333333 0.0 0.3333333333333333 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3333333333333333 \n",
      "0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 \n",
      "[\"!\",\"Hello\",\"it\",\"going\",\"your\",\".\",\"?\",\"name\",\"My\",\"How\",\"What\",\"is\",\"Cooper\",\"Geffen\"]\n"
     ]
    }
   ],
   "source": [
    "# create a dummy object for testing\n",
    "history = 1\n",
    "gen = TextGenerator(\"\",history)\n",
    "\n",
    "# sample corpus\n",
    "text = \"Hello! How is it going? My name is Geffen Cooper. What is your name? \"\n",
    "tl, ml = gen.text_to_list(text)\n",
    "\n",
    "d = gen.gen_word_dist(tl,ml,True)\n",
    "\n",
    "mat,labels = gen.generate_transition_graph(d)\n",
    "print_array(mat)\n",
    "label_str = \"[\"\n",
    "for n in labels:\n",
    "    label_str += '\"{}\"'.format(n) + \",\"\n",
    "print(label_str[:-1] + \"]\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12fed321ce14db8c70d5fd7d87816765955c1efc0c914b111c5a8918a271e9da"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
