{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set up random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "seed = 23\n",
    "rng = default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: This function takes a string of text and turns it into a list where\n",
    "             each element represents  a single text 'token' including punctuation\n",
    "             (These are the states of the markov chain). If desired, we can also\n",
    "             split the text into multitoken elements so that we can consider a\n",
    "             history of tokens (this breaks the markov property but is interesting\n",
    "             to explore). \n",
    "\n",
    "Input: text - A string of text\n",
    "       history_size - the number of tokens per element.\n",
    "\n",
    "Returns: - A list of individual tokens elements in chronological order,\n",
    "         - A list of all the multitoken elements in chronological order\n",
    "           (None if history size is 1).\n",
    "'''\n",
    "\n",
    "def text_to_list(text,history_size=1):\n",
    "    # insert spaces so we can use the split function while keeping punctuation\n",
    "    text = text.replace(\". \",\" . \").replace(\"! \",\" ! \").replace(\"? \",\" ? \").replace(\", \",\" , \")\\\n",
    "               .replace(\"; \",\" ; \").replace(\": \",\" : \").replace(\"\\\"\",\"\").replace(\".\\n\",\" . <NL> \")\\\n",
    "               .replace(\",\\n\",\" , <NL> \").replace(\"!\\n\",\" ! <NL> \").replace(\"?\\n\",\" ? <NL> \")\\\n",
    "               .replace(\":\\n\",\" : <NL> \")\n",
    "\n",
    "    # replace special characters with tokens\n",
    "    text = text.replace(\"\\t\", \" <TAB>\").replace(\"\\n\", \" <NL> \")\n",
    "\n",
    "    # save the end tokens\n",
    "    end_tokens = [\".\", \"?\", \"!\"]\n",
    "\n",
    "    # now split the text using a space as the delimeter\n",
    "    text_list = text.split()\n",
    "\n",
    "    # if each token is a state then we are done\n",
    "    if history_size == 1:\n",
    "        return text_list, None\n",
    "\n",
    "    # for multitoken states, combine items in the list\n",
    "    multitoken_items = []\n",
    "    idx = 0\n",
    "    while idx < len(text_list):\n",
    "        # handle ending punctuation to avoid cases where not at end (e.g. [' . It was'])\n",
    "        if idx+history_size-1 >= len(text_list):\n",
    "            break\n",
    "        # if text_list[idx+history_size-1] in end_tokens:\n",
    "        #     multitoken_items.append(' '.join(text_list[idx:idx+history_size]))\n",
    "        #     idx = idx+history_size\n",
    "        #     continue\n",
    "\n",
    "        # combine items based on state size\n",
    "        multitoken_items.append(' '.join(text_list[idx:idx+history_size]))\n",
    "        idx += 1\n",
    "\n",
    "    return text_list, multitoken_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['My', 'name', 'is', 'Geffen', 'Cooper', '.', 'What', 'is', 'yours', '?'],\n",
       " ['My name',\n",
       "  'name is',\n",
       "  'is Geffen',\n",
       "  'Geffen Cooper',\n",
       "  'Cooper .',\n",
       "  '. What',\n",
       "  'What is',\n",
       "  'is yours',\n",
       "  'yours ?'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"My name is Geffen Cooper. What is yours? \"\n",
    "text_to_list(text,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: This function takes the ordered text list and creates\n",
    "             a markov chain representation from it. Each token represents a state \n",
    "             and the token immediately following represents a potential\n",
    "             next state. We can represent this as a nested dictionary where at\n",
    "             the first level we have all the unique words in the corpus (i.e. the states)\n",
    "             and at the second level we have all the potential next states and\n",
    "             their relative probability of following.\n",
    "             \n",
    "             When the history size is greater than one, we will use multiple words to \n",
    "             determine the next word. This technically breaks the markov property but\n",
    "             will enable more realistic text by considering the history of words instead\n",
    "             of only the current one.\n",
    "\n",
    "Input: token_list - An ordered list of individual text tokens.\n",
    "       history_size - Number of past tokens to consider\n",
    "       multitoken_list - List of multitoken elements for history_size > 1\n",
    "\n",
    "Returns: A nested dictionary representing the Markov Chain. In the case where the\n",
    "         history size is greater than 1, the first level will be all the unique\n",
    "         multitoken elements and the second level will be the possible next individual words.\n",
    "'''\n",
    "def gen_word_dist(token_list, history_size=1,multitoken_list=None):\n",
    "\n",
    "    # case when only consider the current token, markov property is true\n",
    "    if history_size == 1:\n",
    "        # create the first level from the unique tokens (all the states)\n",
    "        unique_tokens = set(token_list)\n",
    "        text_dict = dict.fromkeys(unique_tokens)\n",
    "\n",
    "        # create a nested dictionary for each unique token (all the outgoing states)\n",
    "        for token in text_dict.keys():\n",
    "            text_dict[token] = {}\n",
    "        \n",
    "        # now add the words that follow each unique token\n",
    "        # where the key is the following word and the value is the count\n",
    "        for idx,token in enumerate(token_list[1:]):\n",
    "            try: # try to increment the count of the token\n",
    "                text_dict[token_list[idx]][token] += 1\n",
    "            except KeyError: # otherwise set it as the first occurence\n",
    "                text_dict[token_list[idx]][token] = 1\n",
    "\n",
    "        # now we convert the counts to probabilities\n",
    "        for state in text_dict.keys():\n",
    "            total = sum(text_dict[state].values())\n",
    "            for out_state in text_dict[state].keys():\n",
    "                text_dict[state][out_state] = text_dict[state][out_state]/total\n",
    "                \n",
    "        return text_dict\n",
    "    \n",
    "    # case when past tokens are considered, markov property is broken\n",
    "    else:\n",
    "        end_tokens = [\".\", \"?\", \"!\"]\n",
    "        # create the first level from the unique tokens (all the states)\n",
    "        unique_sequences = set(multitoken_list)\n",
    "        text_dict = dict.fromkeys(unique_sequences)\n",
    "\n",
    "        # create a nested dictionary for each unique token (all the outgoing states)\n",
    "        for sequence in text_dict.keys():\n",
    "            text_dict[sequence] = {}\n",
    "        \n",
    "        # now add the words that follow each unique sequence\n",
    "        # where the key is the following word and the value is the count\n",
    "        token_idx = history_size\n",
    "        for idx,sequence in enumerate(multitoken_list[:-1]):\n",
    "            try: # try to increment the count of the token\n",
    "                text_dict[multitoken_list[idx]][token_list[token_idx]] += 1\n",
    "            except KeyError: # otherwise set it as the first occurence\n",
    "                text_dict[multitoken_list[idx]][token_list[token_idx]] = 1\n",
    "            # if multitoken_list[idx][-1] in end_tokens:\n",
    "            #     token_idx += history_size\n",
    "            # else:\n",
    "            #     token_idx += 1\n",
    "            token_idx += 1\n",
    "\n",
    "        # now we convert the counts to probabilities\n",
    "        for state in text_dict.keys():\n",
    "            total = sum(text_dict[state].values())\n",
    "            for out_state in text_dict[state].keys():\n",
    "                text_dict[state][out_state] = text_dict[state][out_state]/total\n",
    "                \n",
    "        return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tall . <NL>': {'He': 1.0},\n",
       " '<NL> He is': {'very': 1.0},\n",
       " 'very happy .': {},\n",
       " 'He is very': {'tall': 0.5, 'happy': 0.5},\n",
       " 'is very happy': {'.': 1.0},\n",
       " 'very tall .': {'<NL>': 1.0},\n",
       " '. <NL> He': {'is': 1.0},\n",
       " 'is very tall': {'.': 1.0}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"He is very tall.\\nHe is very happy. \"\n",
    "tl, ml = text_to_list(text,3)\n",
    "gen_word_dist(tl,3,ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: This function will take a given state in the markov chain and\n",
    "             select the next state probabilistically.\n",
    "\n",
    "Input: The current state as a dictionary.\n",
    "\n",
    "Returns: The next state.\n",
    "'''\n",
    "\n",
    "def get_next_state(current_state):\n",
    "    out_states = list(current_state.keys())\n",
    "    probs = list(current_state.values())\n",
    "    \n",
    "    probs[0] += 1-sum(probs)\n",
    "    return rng.choice(a=out_states,size=1,p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: This function will step through the generated markov chain\n",
    "             to produce sentences based on the input parameters. It will\n",
    "             format the text as well.\n",
    "\n",
    "Input: The first word (must be in the corpus), the number of desired sentences,\n",
    "       the markov chain dictionary\n",
    "\n",
    "Returns: The formatted output text.\n",
    "'''\n",
    "\n",
    "def generate_text(first_word,num_sentences,markov_chain, history_size=1):\n",
    "    if history_size == 1:\n",
    "        # get the state from the desired first word\n",
    "        curr_state = markov_chain[first_word]\n",
    "\n",
    "        # start the text sequence\n",
    "        text = [first_word]\n",
    "\n",
    "        sentence_count = 0\n",
    "        # keep adding words until we reach the sentence count\n",
    "        while sentence_count < num_sentences:\n",
    "            # get the next word\n",
    "            next_state = get_next_state(curr_state)\n",
    "\n",
    "            # add this word to the text sequence (only get the string)\n",
    "            text.append(next_state.tolist()[0])\n",
    "\n",
    "            # set the current state to the next state (only get the string)\n",
    "            curr_state = markov_chain[next_state[0]]\n",
    "\n",
    "            # check if the sentence ended\n",
    "            if next_state[0] == '.' or next_state[0] == '?' or next_state[0] == '!':\n",
    "                sentence_count += 1\n",
    "\n",
    "        # format the text sequence into a sentence\n",
    "        text_string = \" \".join(text)\n",
    "        text_string = text_string.replace(\" . \",\". \").replace(\" , \",\", \").replace(\" ; \",\"; \").replace(\" ! \",\"! \").replace(\" ? \",\"? \").replace(\" : \",\": \")\n",
    "        text_string = text_string.replace(\"<TAB> \",\"\\t\").replace(\"<NL> \",\"\\n\")\n",
    "        text_string_final = text_string[:-2]+text_string[-1]\n",
    "        return text_string_final\n",
    "    else:\n",
    "        end_tokens = [\".\", \"?\", \"!\"]\n",
    "        # get the state from the desired first word\n",
    "        curr_state = markov_chain[first_word]\n",
    "\n",
    "        # start the text sequence\n",
    "        text = first_word.split()\n",
    "\n",
    "        sentence_count = 0\n",
    "        token_idx = history_size\n",
    "        # keep adding words until we reach the sentence count\n",
    "        while sentence_count < num_sentences:\n",
    "            # get the next word\n",
    "            next_word = get_next_state(curr_state)\n",
    "\n",
    "            # add this word to the text sequence (only get the string)\n",
    "            text.append(next_word.tolist()[0])\n",
    "\n",
    "            # set the current state to the next state (only get the string)\n",
    "            # if text[token_idx] in end_tokens: # when reach end, go to the start of the sentence\n",
    "            #     token_idx += history_size\n",
    "\n",
    "            try:\n",
    "                curr_state = markov_chain[\" \".join(text[token_idx-history_size+1:token_idx+1])]\n",
    "\n",
    "            except KeyError:\n",
    "                print(\"==== Exception ====\")\n",
    "                print(token_idx,text)\n",
    "                print(\"\\n\\n\")\n",
    "                print(text[token_idx-history_size+1])\n",
    "                print(\"\\n\\n\")\n",
    "                print(text[token_idx-history_size+1:token_idx+1])\n",
    "                break\n",
    "            # check if there is no next state\n",
    "            if curr_state == {}:\n",
    "                curr_state = markov_chain[first_word]\n",
    "\n",
    "            # check if the sentence ended\n",
    "            if next_word[0] == '.' or next_word[0] == '?' or next_word[0] == '!':\n",
    "                sentence_count += 1\n",
    "            \n",
    "            token_idx += 1\n",
    "\n",
    "        # format the text sequence into a sentence\n",
    "        text_string = \" \".join(text)\n",
    "        text_string = text_string.replace(\" . \",\". \").replace(\" , \",\", \").replace(\" ; \",\"; \").replace(\" ! \",\"! \").replace(\" ? \",\"? \").replace(\" : \",\": \")\n",
    "        text_string = text_string.replace(\"<TAB> \",\"\\t\").replace(\"<NL> \",\"\\n\")\n",
    "        text_string_final = text_string[:-2]+text_string[-1]\n",
    "        return text_string_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not have a': {'a': 1.0}, 'over email if': {'if': 1.0}, 'then move to': {'to': 1.0}, ': <NL> <NL>': {'<NL>': 1.0}, 'office hour at': {'at': 1.0}, 'the first 3': {'3': 1.0}, 'a holiday .': {'.': 1.0}, 'Monday is a': {'a': 1.0}, 'still hold the': {'the': 1.0}, 'a class today': {'today': 1.0}, 'holiday , we': {'we': 1.0}, 'email if needed': {'needed': 1.0}, 'hours on Tuesday': {'Tuesday': 1.0}, 'on Monday and': {'and': 1.0}, '. You can': {'can': 1.0}, 'The final project': {'project': 1.0}, 'plots , and': {'and': 1.0}, '. The final': {'final': 1.0}, 'a real data': {'data': 1.0}, 'share your code': {'code': 1.0}, 'notes up to': {'to': 1.0}, 'material in lecture': {'lecture': 1.0}, 'laptop but you': {'you': 1.0}, 'will be attending': {'attending': 1.0}, 'set you find': {'find': 1.0}, 'survey paper discussing': {'discussing': 1.0}, 'continue on Wednesday': {'Wednesday': 1.0}, 'a 3-4 page': {'page': 1.0}, 'have office hours': {'hours': 1.0}, '. We are': {'are': 1.0}, 'show plots ,': {',': 1.0}, 'class . It': {'It': 1.0}, 'Hope that you': {'you': 1.0}, 'the last office': {'office': 1.0}, 'to answer questions': {'questions': 1.0}, 'few requests to': {'to': 1.0}, 'lab with report': {'report': 1.0}, 'now submit it': {'it': 1.0}, 'will review some': {'some': 1.0}, 'do comparisons ,': {',': 1.0}, 'code together with': {'with': 1.0}, 'Since Monday is': {'is': 1.0}, 'Monday over zoom': {'zoom': 1.0}, '2) read and': {'and': 1.0}, 'we are going': {'going': 1.0}, 'a related machine': {'machine': 1.0}, 'would be good': {'good': 1.0}, 'will still hold': {'hold': 1.0}, 'We will still': {'still': 1.0}, 'were a few': {'few': 1.0}, 'report ; 2)': {'2)': 1.0}, 'in a report': {'report': 1.0}, 'materials for midterm': {'midterm': 1.0}, '. Please note': {'note': 1.0}, 'Monday and Mark': {'Mark': 1.0}, 'design a new': {'new': 1.0}, ', and share': {'share': 1.0}, 'Markov Models .': {'.': 1.0}, 'class on a': {'a': 1.0}, 'since my office': {'office': 1.0}, '11 AM on': {'on': 1.0}, '. <NL> <NL>': {'<NL>': 1.0}, '11 the following': {'following': 1.0}, 'learning research paper': {'paper': 1.0}, 'out of town': {'town': 1.0}, 'Learning . I': {'I': 1.0}, 'are going to': {'to': 1.0}, 'note that the': {'the': 1.0}, 'at 11 AM': {'AM': 1.0}, 'a summary in': {'in': 1.0}, 'the class in': {'in': 1.0}, '. This deadline': {'deadline': 1.0}, 'at 3:30 but': {'but': 1.0}, 'there were a': {'a': 1.0}, 'on Tuesday 31st': {'31st': 1.0}, 'move to Markov': {'Markov': 1.0}, 'your laptop but': {'but': 1.0}, 'have the office': {'office': 1.0}, 'since Monday 30th': {'30th': 1.0}, 'learned in the': {'the': 1.0}, 'are well .': {'.': 1.0}, 'excluding Hidden Markov': {'Markov': 1.0}, '<NL> Note that': {'that': 1.0}, 'a report or': {'or': 1.0}, 'only allowed to': {'to': 1.0}, 'the weather .': {'.': 1.0}, 'first 3 HWs': {'HWs': 1.0}, 'by Mark on': {'on': 1.0}, 'find online and': {'and': 1.0}, 'by Tuesday 11:59': {'11:59': 1.0}, '3-4 page report': {'report': 1.0}, '<NL> https://ucsb.zoom.us/j/88570252738?pwd=Rk44a09vYktzdWFVTE5UazBhZ3BmUT09 <NL>': {'<NL>': 1.0}, 'You can bring': {'bring': 1.0}, 'deadline , you': {'you': 1.0}, 'comparisons , conclusions': {'conclusions': 1.0}, 'algorithm(s) learned in': {'in': 1.0}, 'Please note that': {'that': 1.0}, 'data set you': {'you': 1.0}, 'up to and': {'and': 1.0}, 'some materials for': {'for': 1.0}, '5/23 in class': {'class': 1.0}, '; we will': {'will': 1.0}, 'is on Monday': {'Monday': 1.0}, 'and share your': {'your': 1.0}, 'all , <NL>': {'<NL>': 1.0}, 'going to have': {'have': 1.0}, 'to use the': {'the': 1.0}, 'the office hour': {'hour': 1.0}, 'I think it': {'it': 1.0}, 'the following week': {'week': 1.0}, 'submit it by': {'by': 1.0}, 'not be extended': {'extended': 1.0}, 'have a lecture': {'lecture': 1.0}, 'AM . The': {'The': 1.0}, 'at the level': {'level': 1.0}, 'hour on Monday': {'Monday': 1.0}, '<NL> Since there': {'there': 1.0}, 'the notes for': {'for': 1.0}, 'class today ;': {';': 1.0}, '<NL> <NL> Note': {'Note': 1.0}, 'at 9:45-10:45 AM': {'AM': 1.0}, 'the exam .': {'.': 1.0}, 'taught by Mark': {'Mark': 1.0}, 'write a summary': {'summary': 1.0}, '<NL> <NL> Best': {'Best': 1.0}, '9:45-10:45 AM .': {'.': 1.0}, '<NL> Since Monday': {'Monday': 1.0}, 'bit under the': {'the': 1.0}, 'research paper and': {'and': 1.0}, 'your code together': {'together': 1.0}, 'on Monday :': {':': 1.0}, 'not have an': {'an': 1.0}, 'at 11 the': {'the': 1.0}, 'for the office': {'office': 1.0}, 'a new lab': {'lab': 1.0}, 'the material on': {'on': 1.0}, 'Processes . Please': {'Please': 1.0}, 'attending a conference': {'conference': 1.0}, 'requests to extend': {'extend': 1.0}, 'summary in a': {'a': 1.0}, 'well . Given': {'Given': 1.0}, 'will have office': {'office': 1.0}, '<NL> I will': {'will': 1.0}, 'Monday 5/23 in': {'in': 1.0}, 'covers the first': {'first': 1.0}, 'in person at': {'at': 1.0}, 'As you know': {'know': 1.0}, 'will have the': {'the': 1.0}, 'an office hour': {'hour': 1.0}, 'can bring your': {'your': 1.0}, 'bring your laptop': {'laptop': 1.0}, 'algorithm , results': {'results': 1.0}, 'potential Covid-19 exposure': {'exposure': 1.0}, 'types of projects': {'projects': 1.0}, 'Wednesday June 8th': {'8th': 1.0}, ', we will': {'will': 1.0}, 'questions over email': {'email': 1.0}, '. I am': {'am': 1.0}, 'of projects :': {':': 1.0}, 'June 8th 11:59': {'11:59': 1.0}, 'strengths and weaknesses': {'weaknesses': 1.0}, '<NL> <NL> Since': {'Since': 1.0}, 'to and excluding': {'excluding': 1.0}, 'use the notes': {'notes': 1.0}, 'PM . This': {'This': 1.0}, 'office is quite': {'quite': 1.0}, 'is a holiday': {'holiday': 1.0}, 'paper discussing the': {'the': 1.0}, 'where I will': {'will': 1.0}, '31st at 11': {'11': 1.0}, ', we are': {'are': 1.0}, 'a few requests': {'requests': 1.0}, 'https://ucsb.zoom.us/j/88570252738?pwd=Rk44a09vYktzdWFVTE5UazBhZ3BmUT09 <NL> <NL>': {'<NL>': 1.0}, 'exam . We': {'We': 1.0}, 'Since there were': {'were': 1.0}, 'you can use': {'use': 1.0}, 'online and do': {'do': 1.0}, 'have an office': {'office': 1.0}, 'holiday . <NL>': {'<NL>': 1.0}, 'be attending a': {'a': 1.0}, 'can now submit': {'submit': 1.0}, '11:59 PM .': {'.': 1.0}, 'to Markov Decision': {'Decision': 1.0}, 'Q Learning .': {'.': 1.0}, 'Monday 30th is': {'is': 1.0}, 'last office hour': {'hour': 1.0}, 'We will have': {'have': 1.0}, 'masks . Here': {'Here': 1.0}, 'with a 3-4': {'3-4': 1.0}, 'have the last': {'last': 1.0}, 'are only allowed': {'allowed': 1.0}, 'the recent potential': {'potential': 1.0}, 'hour on Tuesday': {'Tuesday': 1.0}, 'extended . <NL>': {'<NL>': 1.0}, 'We will not': {'not': 1.0}, 'available to answer': {'answer': 1.0}, 'is due Wednesday': {'Wednesday': 1.0}, 'office hour on': {'on': 1.0}, 'Covid-19 exposure in': {'in': 1.0}, '1) implementing ML': {'ML': 1.0}, 'conclusions , show': {'show': 1.0}, 'lecture on Wednesday': {'Wednesday': 1.0}, 'Models . <NL>': {'<NL>': 1.0}, 'in class ,': {',': 1.0}, 'to extend the': {'the': 1.0}, '<NL> <NL> ----': {'----': 1.0}, 'but you are': {'are': 1.0}, 'is going to': {'to': 1.0}, 'today ; we': {'we': 1.0}, 'Hidden Markov Models': {'Models': 1.0}, 'zoom since my': {'my': 1.0}, 'I feel a': {'a': 1.0}, 'notes for the': {'the': 1.0}, 'you know ,': {',': 1.0}, 'there can be': {'be': 1.0}, ', show plots': {'plots': 1.0}, '. We will': {'will': 1.0}, 'Decision Processes .': {'.': 1.0}, ', there can': {'can': 1.0}, 'Tuesday at 9:45-10:45': {'9:45-10:45': 1.0}, 'two types of': {'of': 1.0}, 'on Monday over': {'over': 1.0}, 'extend the HW': {'HW': 1.0}, 'if needed .': {'.': 1.0}, 'on a real': {'real': 1.0}, 'level of labs': {'labs': 1.0}, 'report is due': {'due': 1.0}, 'week . We': {'We': 1.0}, ', conclusions ,': {',': 1.0}, 'HWs and material': {'material': 1.0}, 'read and study': {'study': 1.0}, 'Given the recent': {'recent': 1.0}, 'exam on Monday': {'Monday': 1.0}, 'on Monday 5/23': {'5/23': 1.0}, '; 2) read': {'read': 1.0}, 'midterm and then': {'then': 1.0}, 'my office is': {'is': 1.0}, 'labs 2-4 so': {'so': 1.0}, 'next week .': {'.': 1.0}, 'in the class': {'class': 1.0}, '. This can': {'can': 1.0}, 'the algorithm ,': {',': 1.0}, 'report . This': {'This': 1.0}, 'you are only': {'only': 1.0}, 'needed . We': {'We': 1.0}, 'I will be': {'be': 1.0}, 'basically design a': {'a': 1.0}, 'week since Monday': {'Monday': 1.0}, 'town next week': {'week': 1.0}, 'lecture notes up': {'up': 1.0}, 'projects : 1)': {'1)': 1.0}, 'conference out of': {'of': 1.0}, 'class in person': {'person': 1.0}, '<NL> Ramtin .': {'.': 1.0}, 'and material in': {'in': 1.0}, 'in lecture notes': {'notes': 1.0}, 'it by Tuesday': {'Tuesday': 1.0}, 'This deadline will': {'will': 1.0}, '<NL> <NL> I': {'I': 1.0}, 'Monday : <NL>': {'<NL>': 1.0}, 'exposure in class': {'class': 1.0}, 'on Q Learning': {'Learning': 1.0}, 'Tuesday 31st at': {'at': 1.0}, 'midterm is on': {'on': 1.0}, 'is the zoom': {'zoom': 1.0}, 'Hi all ,': {',': 1.0}, '<NL> I feel': {'feel': 1.0}, 'link for the': {'the': 1.0}, 'feel a bit': {'bit': 1.0}, 'of labs 2-4': {'2-4': 1.0}, 'the zoom link': {'link': 1.0}, 'paper and write': {'write': 1.0}, 'answer questions over': {'over': 1.0}, 'quite small .': {'.': 1.0}, 'Monday . You': {'You': 1.0}, 'a holiday ,': {',': 1.0}, 'final project report': {'report': 1.0}, '2-4 so basically': {'basically': 1.0}, ', results ,': {',': 1.0}, 'we will have': {'have': 1.0}, 'have a class': {'class': 1.0}, 'ML algorithm(s) learned': {'learned': 1.0}, 'be two types': {'types': 1.0}, 'midterm exam on': {'on': 1.0}, 'and then move': {'move': 1.0}, ', strengths and': {'and': 1.0}, '<NL> Best ,': {',': 1.0}, 'and study a': {'a': 1.0}, 'administer the exam': {'exam': 1.0}, 'am available to': {'to': 1.0}, 'class , we': {'we': 1.0}, 'It covers the': {'the': 1.0}, ': 1) implementing': {'implementing': 1.0}, 'review some materials': {'materials': 1.0}, 'you can now': {'now': 1.0}, 'HW deadline ,': {',': 1.0}, 'for midterm and': {'and': 1.0}, 'PM . As': {'As': 1.0}, 'that you can': {'can': 1.0}, 'on Monday .': {'.': 1.0}, 'is quite small': {'small': 1.0}, 'hour at 11': {'11': 1.0}, 'can use the': {'the': 1.0}, 'discussing the algorithm': {'algorithm': 1.0}, 'be at the': {'the': 1.0}, 'machine learning research': {'research': 1.0}, 'new lab with': {'with': 1.0}, 'a lecture on': {'on': 1.0}, 'study a related': {'related': 1.0}, '3:30 but I': {'I': 1.0}, ', you can': {'can': 1.0}, 'Wednesday where I': {'I': 1.0}, 'think it would': {'would': 1.0}, 'the level of': {'of': 1.0}, 'use the material': {'material': 1.0}, 'related machine learning': {'learning': 1.0}, 'you are well': {'well': 1.0}, 'material on Gauchospace': {'Gauchospace': 1.0}, 'on Tuesday at': {'at': 1.0}, 'we will continue': {'continue': 1.0}, 'that the midterm': {'midterm': 1.0}, '<NL> ---- <NL>': {'<NL>': 1.0}, 'We are going': {'going': 1.0}, 'will continue on': {'on': 1.0}, 'so basically design': {'design': 1.0}, 'page report .': {'.': 1.0}, 'the midterm is': {'is': 1.0}, 'wear masks .': {'.': 1.0}, '<NL> <NL> Ramtin': {'Ramtin': 1.0}, 'AM on Monday': {'Monday': 1.0}, 'will not have': {'have': 1.0}, 'recent potential Covid-19': {'Covid-19': 1.0}, 'to administer the': {'the': 1.0}, 'can be at': {'at': 1.0}, '30th is a': {'a': 1.0}, 'a bit under': {'under': 1.0}, 'following week since': {'since': 1.0}, 'due Wednesday June': {'June': 1.0}, 'allowed to use': {'use': 1.0}, 'Ramtin . <NL>': {'<NL>': 1.0}, 'to have a': {'a': 1.0}, 'good to wear': {'wear': 1.0}, 'the HW deadline': {'deadline': 1.0}, 'deadline will not': {'not': 1.0}, 'and do comparisons': {'comparisons': 1.0}, 'and weaknesses .': {'.': 1.0}, '---- <NL> Hi': {'Hi': 1.0}, 'that you are': {'are': 1.0}, 'know , there': {'there': 1.0}, 'be extended .': {'.': 1.0}, 'under the weather': {'weather': 1.0}, 'Wednesday taught by': {'by': 1.0}, 'results , strengths': {'strengths': 1.0}, 'Mark on Q': {'Q': 1.0}, 'to have the': {'the': 1.0}, '<NL> <NL> https://ucsb.zoom.us/j/88570252738?pwd=Rk44a09vYktzdWFVTE5UazBhZ3BmUT09': {'https://ucsb.zoom.us/j/88570252738?pwd=Rk44a09vYktzdWFVTE5UazBhZ3BmUT09': 1.0}, 'to wear masks': {'masks': 1.0}, 'Tuesday 11:59 PM': {'PM': 1.0}, '. It covers': {'covers': 1.0}, 'This can be': {'be': 1.0}, 'going to administer': {'administer': 1.0}, 'it would be': {'be': 1.0}, 'or survey paper': {'paper': 1.0}, 'implementing ML algorithm(s)': {'algorithm(s)': 1.0}, 'report or survey': {'survey': 1.0}, 'over zoom since': {'since': 1.0}, 'for the midterm': {'midterm': 1.0}, 'on Wednesday taught': {'taught': 1.0}, 'and excluding Hidden': {'Hidden': 1.0}, 'office hours on': {'on': 1.0}, 'be good to': {'to': 1.0}, 'Best , <NL>': {'<NL>': 1.0}, 'weather . We': {'We': 1.0}, 'Gauchospace . <NL>': {'<NL>': 1.0}, ', <NL> <NL>': {'<NL>': 1.0}, '3 HWs and': {'and': 1.0}, 'with report ;': {';': 1.0}, 'you find online': {'online': 1.0}, 'the midterm exam': {'exam': 1.0}, '. As you': {'you': 1.0}, 'the class on': {'on': 1.0}, '<NL> <NL> Hope': {'Hope': 1.0}, 'Here is the': {'the': 1.0}, 'person at 3:30': {'3:30': 1.0}, 'real data set': {'set': 1.0}, 'and Mark is': {'is': 1.0}, 'but I think': {'think': 1.0}, 'will not be': {'be': 1.0}, 'together with a': {'a': 1.0}, 'zoom link for': {'for': 1.0}, 'of town next': {'next': 1.0}, '8th 11:59 PM': {'PM': 1.0}, 'a conference out': {'out': 1.0}, 'I am available': {'available': 1.0}, 'on Wednesday where': {'where': 1.0}, 'project report is': {'is': 1.0}, '. Here is': {'is': 1.0}, 'Mark is going': {'going': 1.0}, 'hold the class': {'class': 1.0}, 'on Gauchospace .': {'.': 1.0}, 'Note that you': {'you': 1.0}, 'weaknesses . <NL>': {'<NL>': 1.0}, '. Given the': {'the': 1.0}, 'in class .': {'.': 1.0}, 'I will review': {'review': 1.0}, 'Markov Decision Processes': {'Processes': 1.0}, 'can be two': {'two': 1.0}, '<NL> Hope that': {'that': 1.0}, 'and write a': {'a': 1.0}, 'small . We': {'We': 1.0}, '<NL> Hi all': {'all': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "with open('1789-04-30-first-inaugural-address.txt',encoding='utf-8') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "with open('2017-01-20-inaugural-address.txt',encoding='utf-8') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "with open('email.txt',encoding='utf-8') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "#d = gen_word_dist(text_to_list(contents)[0])\n",
    "tl,ml = text_to_list(contents,3)\n",
    "# print(tl)\n",
    "# print(\"\\n\\n\")\n",
    "# print(ml)\n",
    "d = gen_word_dist(tl,2,ml)\n",
    "print(d)\n",
    "# print(generate_text(\"Hi all ,\",2,d,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12fed321ce14db8c70d5fd7d87816765955c1efc0c914b111c5a8918a271e9da"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
