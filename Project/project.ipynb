{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "seed = 23\n",
    "rng = default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['b'], dtype='<U1')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = ['a','b','c']\n",
    "p = [0.1,0.5,0.39]\n",
    "p[0] += 1-sum(p)\n",
    "rng.choice(a=values,size=1,p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: This function takes a string of text and turns it into a list where\n",
    "             each element is a word or punctuation.\n",
    "\n",
    "Input: A string of text.\n",
    "\n",
    "Returns: A list of the individual words and periods in order.\n",
    "'''\n",
    "\n",
    "def text_to_list(text):\n",
    "    # insert spaces so we can use the split function while keeping punctuation\n",
    "    text = text.replace(\". \",\" . \").replace(\", \",\" , \").replace(\"; \",\" ; \").replace(\"! \",\" ! \").replace(\"? \",\" ? \").replace(\": \",\" : \").replace(\"\\\"\",\"\")\n",
    "\n",
    "    # replace special characters with tokens\n",
    "    text = text.replace(\"\\t\", \" <TAB>\").replace(\"\\n\", \" <NL> \")\n",
    "\n",
    "    # now split the text using a space as the delimeter\n",
    "    items = text.split()\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: This function takes the ordered list of text 'tokens' and creates\n",
    "             a markov chain representation from it. Each token represents a \n",
    "             state and the token immediately following represents a potential\n",
    "             next state. We can represent this as a nested dictionary where at\n",
    "             the first level we have all the unique words in the corpus (states) and at\n",
    "             the second level we have all the potential next states and their\n",
    "             relative probability of following.\n",
    "\n",
    "Input: A list of text tokens.\n",
    "\n",
    "Returns: A nested dictionary representing the Markov Chain.\n",
    "'''\n",
    "def gen_word_dist(token_list):\n",
    "    # create the first level from the unique tokens (all the states)\n",
    "    unique_tokens = set(token_list)\n",
    "    text_dict = dict.fromkeys(unique_tokens)\n",
    "\n",
    "    # create a nested dictionary for each unique token (all the outgoing states)\n",
    "    for token in text_dict.keys():\n",
    "        text_dict[token] = {}\n",
    "    \n",
    "    # now add the words that follow each unique token\n",
    "    # where the key is the following word and the value is the count\n",
    "    for idx,token in enumerate(token_list[1:]):\n",
    "        try: # try to increment the count of the token\n",
    "            text_dict[token_list[idx]][token] += 1\n",
    "        except KeyError: # otherwise set it as the first occurence\n",
    "            text_dict[token_list[idx]][token] = 1\n",
    "\n",
    "    # now we convert the counts to probabilities\n",
    "    for state in text_dict.keys():\n",
    "        total = sum(text_dict[state].values())\n",
    "        for out_state in text_dict[state].keys():\n",
    "            text_dict[state][out_state] = text_dict[state][out_state]/total\n",
    "            \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: This function will take a given state in the markov chain and\n",
    "             select the next state probabilistically.\n",
    "\n",
    "Input: The current state as a dictionary.\n",
    "\n",
    "Returns: The next state.\n",
    "'''\n",
    "\n",
    "def get_next_state(current_state):\n",
    "    out_states = list(current_state.keys())\n",
    "    probs = list(current_state.values())\n",
    "    \n",
    "    probs[0] += 1-sum(probs)\n",
    "    return rng.choice(a=out_states,size=1,p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: This function will step through the generated markov chain\n",
    "             to produce sentences based on the input parameters. It will\n",
    "             format the text as well.\n",
    "\n",
    "Input: The first word (must be in the corpus), the number of desired sentences,\n",
    "       the markov chain dictionary\n",
    "\n",
    "Returns: The formatted output text.\n",
    "'''\n",
    "\n",
    "def generate_text(first_word,num_sentences,markov_chain):\n",
    "    # get the state from the desired first word\n",
    "    curr_state = markov_chain[first_word]\n",
    "\n",
    "    # start the text sequence\n",
    "    text = [first_word]\n",
    "\n",
    "    sentence_count = 0\n",
    "    # keep adding words until we reach the sentence count\n",
    "    while sentence_count < num_sentences:\n",
    "        # get the next word\n",
    "        next_state = get_next_state(curr_state)\n",
    "\n",
    "        # add this word to the text sequence (only get the string)\n",
    "        text.append(next_state.tolist()[0])\n",
    "\n",
    "        # set the current state to the next state (only get the string)\n",
    "        curr_state = markov_chain[next_state[0]]\n",
    "\n",
    "        # check if the sentence ended\n",
    "        if next_state[0] == '.' or next_state[0] == '?' or next_state[0] == '!':\n",
    "            sentence_count += 1\n",
    "\n",
    "    # format the text sequence into a sentence\n",
    "    text_string = \" \".join(text)\n",
    "    text_string = text_string.replace(\" . \",\". \").replace(\" , \",\", \").replace(\" ; \",\"; \").replace(\" ! \",\"! \").replace(\" ? \",\"? \").replace(\" : \",\": \")\n",
    "    text_string = text_string.replace(\"<TAB>\",\"\\t\").replace(\"<NL>\",\"\\n\")\n",
    "    text_string_final = text_string[:-2]+text_string[-1]\n",
    "    return text_string_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The circumstances, on another, who, and advantageously promoted. Instead of this transcendent proof, nor those of public summons, are none under which I must depend.\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, my name is Geffen! What is Yours?\\n hi\"\n",
    "with open('1789-04-30-first-inaugural-address.txt') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "d = gen_word_dist(text_to_list(contents))\n",
    "print(generate_text(\"The\",2,d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140713966737536\n"
     ]
    }
   ],
   "source": [
    "with open('1789-04-30-first-inaugural-address.txt') as f:\n",
    "    contents = f.read()\n",
    "#print(text_to_list(contents))\n",
    "d = dict.fromkeys(set(text_to_list(contents)))\n",
    "print(id(d['the']))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12fed321ce14db8c70d5fd7d87816765955c1efc0c914b111c5a8918a271e9da"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
