{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set up random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "seed = 23\n",
    "rng = default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: This function takes a string of text and turns it into a list where\n",
    "             each element represents  a single text 'token' including punctuation\n",
    "             (These are the states of the markov chain). If desired, we can also\n",
    "             split the text into multitoken elements so that we can consider a\n",
    "             history of tokens (this breaks the markov property but is interesting\n",
    "             to explore). \n",
    "\n",
    "Input: text - A string of text\n",
    "       history_size - the number of tokens per element.\n",
    "\n",
    "Returns: - A list of individual tokens elements in chronological order,\n",
    "         - A list of all the multitoken elements in chronological order\n",
    "           (None if history size is 1).\n",
    "'''\n",
    "\n",
    "def text_to_list(text,history_size=1):\n",
    "    # insert spaces so we can use the split function while keeping punctuation\n",
    "    text = text.replace(\". \",\" . \").replace(\"! \",\" ! \").replace(\"? \",\" ? \").replace(\", \",\" , \")\\\n",
    "               .replace(\"; \",\" ; \").replace(\": \",\" : \").replace(\"\\\"\",\"\").replace(\".\\n\",\" . <NL> \")\\\n",
    "               .replace(\",\\n\",\" , <NL> \").replace(\"!\\n\",\" ! <NL> \").replace(\"?\\n\",\" ? <NL> \")\\\n",
    "               .replace(\":\\n\",\" : <NL> \")\n",
    "\n",
    "    # replace special characters with tokens\n",
    "    text = text.replace(\"\\t\", \" <TAB>\").replace(\"\\n\", \" <NL> \")\n",
    "\n",
    "    # save the end tokens\n",
    "    end_tokens = [\".\", \"?\", \"!\"]\n",
    "\n",
    "    # now split the text using a space as the delimeter\n",
    "    text_list = text.split()\n",
    "\n",
    "    # if each token is a state then we are done\n",
    "    if history_size == 1:\n",
    "        return text_list, None\n",
    "\n",
    "    # for multitoken states, combine items in the list\n",
    "    multitoken_items = []\n",
    "    idx = 0\n",
    "    while idx < len(text_list):\n",
    "        # handle ending punctuation to avoid cases where not at end (e.g. [' . It was'])\n",
    "        if idx+history_size-1 >= len(text_list):\n",
    "            break\n",
    "        # if text_list[idx+history_size-1] in end_tokens:\n",
    "        #     multitoken_items.append(' '.join(text_list[idx:idx+history_size]))\n",
    "        #     idx = idx+history_size\n",
    "        #     continue\n",
    "\n",
    "        # combine items based on state size\n",
    "        multitoken_items.append(' '.join(text_list[idx:idx+history_size]))\n",
    "        idx += 1\n",
    "\n",
    "    return text_list, multitoken_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['My', 'name', 'is', 'Geffen', 'Cooper', '.', 'What', 'is', 'yours', '?'],\n",
       " ['My name',\n",
       "  'name is',\n",
       "  'is Geffen',\n",
       "  'Geffen Cooper',\n",
       "  'Cooper .',\n",
       "  '. What',\n",
       "  'What is',\n",
       "  'is yours',\n",
       "  'yours ?'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"My name is Geffen Cooper. What is yours? \"\n",
    "text_to_list(text,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: This function takes the ordered text list and creates\n",
    "             a markov chain representation from it. Each token represents a state \n",
    "             and the token immediately following represents a potential\n",
    "             next state. We can represent this as a nested dictionary where at\n",
    "             the first level we have all the unique words in the corpus (i.e. the states)\n",
    "             and at the second level we have all the potential next states and\n",
    "             their relative probability of following.\n",
    "             \n",
    "             When the history size is greater than one, we will use multiple words to \n",
    "             determine the next word. This technically breaks the markov property but\n",
    "             will enable more realistic text by considering the history of words instead\n",
    "             of only the current one.\n",
    "\n",
    "Input: token_list - An ordered list of individual text tokens.\n",
    "       history_size - Number of past tokens to consider\n",
    "       multitoken_list - List of multitoken elements for history_size > 1\n",
    "\n",
    "Returns: A nested dictionary representing the Markov Chain. In the case where the\n",
    "         history size is greater than 1, the first level will be all the unique\n",
    "         multitoken elements and the second level will be the possible next individual words.\n",
    "'''\n",
    "def gen_word_dist(token_list, history_size=1,multitoken_list=None):\n",
    "\n",
    "    # case when only consider the current token, markov property is true\n",
    "    if history_size == 1:\n",
    "        # create the first level from the unique tokens (all the states)\n",
    "        unique_tokens = set(token_list)\n",
    "        text_dict = dict.fromkeys(unique_tokens)\n",
    "\n",
    "        # create a nested dictionary for each unique token (all the outgoing states)\n",
    "        for token in text_dict.keys():\n",
    "            text_dict[token] = {}\n",
    "        \n",
    "        # now add the words that follow each unique token\n",
    "        # where the key is the following word and the value is the count\n",
    "        for idx,token in enumerate(token_list[1:]):\n",
    "            try: # try to increment the count of the token\n",
    "                text_dict[token_list[idx]][token] += 1\n",
    "            except KeyError: # otherwise set it as the first occurence\n",
    "                text_dict[token_list[idx]][token] = 1\n",
    "\n",
    "        # now we convert the counts to probabilities\n",
    "        for state in text_dict.keys():\n",
    "            total = sum(text_dict[state].values())\n",
    "            for out_state in text_dict[state].keys():\n",
    "                text_dict[state][out_state] = text_dict[state][out_state]/total\n",
    "                \n",
    "        return text_dict\n",
    "    \n",
    "    # case when past tokens are considered, markov property is broken\n",
    "    else:\n",
    "        end_tokens = [\".\", \"?\", \"!\"]\n",
    "        # create the first level from the unique tokens (all the states)\n",
    "        unique_sequences = set(multitoken_list)\n",
    "        text_dict = dict.fromkeys(unique_sequences)\n",
    "\n",
    "        # create a nested dictionary for each unique token (all the outgoing states)\n",
    "        for sequence in text_dict.keys():\n",
    "            text_dict[sequence] = {}\n",
    "        \n",
    "        # now add the words that follow each unique sequence\n",
    "        # where the key is the following word and the value is the count\n",
    "        token_idx = history_size\n",
    "        for idx,sequence in enumerate(multitoken_list[:-1]):\n",
    "            try: # try to increment the count of the token\n",
    "                text_dict[multitoken_list[idx]][token_list[token_idx]] += 1\n",
    "            except KeyError: # otherwise set it as the first occurence\n",
    "                text_dict[multitoken_list[idx]][token_list[token_idx]] = 1\n",
    "            # if multitoken_list[idx][-1] in end_tokens:\n",
    "            #     token_idx += history_size\n",
    "            # else:\n",
    "            #     token_idx += 1\n",
    "            token_idx += 1\n",
    "\n",
    "        # now we convert the counts to probabilities\n",
    "        for state in text_dict.keys():\n",
    "            total = sum(text_dict[state].values())\n",
    "            for out_state in text_dict[state].keys():\n",
    "                text_dict[state][out_state] = text_dict[state][out_state]/total\n",
    "                \n",
    "        return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tall . <NL>': {'He': 1.0},\n",
       " '<NL> He is': {'very': 1.0},\n",
       " 'very happy .': {},\n",
       " 'He is very': {'tall': 0.5, 'happy': 0.5},\n",
       " 'is very happy': {'.': 1.0},\n",
       " 'very tall .': {'<NL>': 1.0},\n",
       " '. <NL> He': {'is': 1.0},\n",
       " 'is very tall': {'.': 1.0}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"He is very tall.\\nHe is very happy. \"\n",
    "tl, ml = text_to_list(text,3)\n",
    "gen_word_dist(tl,3,ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: This function will take a given state in the markov chain and\n",
    "             select the next state probabilistically.\n",
    "\n",
    "Input: The current state as a dictionary.\n",
    "\n",
    "Returns: The next state.\n",
    "'''\n",
    "\n",
    "def get_next_state(current_state):\n",
    "    out_states = list(current_state.keys())\n",
    "    probs = list(current_state.values())\n",
    "    \n",
    "    probs[0] += 1-sum(probs)\n",
    "    return rng.choice(a=out_states,size=1,p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: This function will step through the generated markov chain\n",
    "             to produce sentences based on the input parameters. It will\n",
    "             format the text as well.\n",
    "\n",
    "Input: The first word (must be in the corpus), the number of desired sentences,\n",
    "       the markov chain dictionary\n",
    "\n",
    "Returns: The formatted output text.\n",
    "'''\n",
    "\n",
    "def generate_text(first_word,num_sentences,markov_chain, history_size=1):\n",
    "    if history_size == 1:\n",
    "        # get the state from the desired first word\n",
    "        curr_state = markov_chain[first_word]\n",
    "\n",
    "        # start the text sequence\n",
    "        text = [first_word]\n",
    "\n",
    "        sentence_count = 0\n",
    "        # keep adding words until we reach the sentence count\n",
    "        while sentence_count < num_sentences:\n",
    "            # get the next word\n",
    "            next_state = get_next_state(curr_state)\n",
    "\n",
    "            # add this word to the text sequence (only get the string)\n",
    "            text.append(next_state.tolist()[0])\n",
    "\n",
    "            # set the current state to the next state (only get the string)\n",
    "            curr_state = markov_chain[next_state[0]]\n",
    "\n",
    "            # check if the sentence ended\n",
    "            if next_state[0] == '.' or next_state[0] == '?' or next_state[0] == '!':\n",
    "                sentence_count += 1\n",
    "\n",
    "        # format the text sequence into a sentence\n",
    "        text_string = \" \".join(text)\n",
    "        text_string = text_string.replace(\" . \",\". \").replace(\" , \",\", \").replace(\" ; \",\"; \").replace(\" ! \",\"! \").replace(\" ? \",\"? \").replace(\" : \",\": \")\n",
    "        text_string = text_string.replace(\"<TAB> \",\"\\t\").replace(\"<NL> \",\"\\n\")\n",
    "        text_string_final = text_string[:-2]+text_string[-1]\n",
    "        return text_string_final\n",
    "    else:\n",
    "        end_tokens = [\".\", \"?\", \"!\"]\n",
    "        # get the state from the desired first word\n",
    "        curr_state = markov_chain[first_word]\n",
    "\n",
    "        # start the text sequence\n",
    "        text = first_word.split()\n",
    "\n",
    "        sentence_count = 0\n",
    "        token_idx = history_size\n",
    "        # keep adding words until we reach the sentence count\n",
    "        while sentence_count < num_sentences:\n",
    "            # get the next word\n",
    "            next_word = get_next_state(curr_state)\n",
    "\n",
    "            # add this word to the text sequence (only get the string)\n",
    "            text.append(next_word.tolist()[0])\n",
    "\n",
    "            # set the current state to the next state (only get the string)\n",
    "            # if text[token_idx] in end_tokens: # when reach end, go to the start of the sentence\n",
    "            #     token_idx += history_size\n",
    "\n",
    "            try:\n",
    "                curr_state = markov_chain[\" \".join(text[token_idx-history_size+1:token_idx+1])]\n",
    "\n",
    "            except KeyError:\n",
    "                print(\"==== Exception ====\")\n",
    "                print(token_idx,text)\n",
    "                print(\"\\n\\n\")\n",
    "                print(text[token_idx-history_size+1])\n",
    "                print(\"\\n\\n\")\n",
    "                print(text[token_idx-history_size+1:token_idx+1])\n",
    "                break\n",
    "            # check if there is no next state\n",
    "            if curr_state == {}:\n",
    "                curr_state = markov_chain[first_word]\n",
    "\n",
    "            # check if the sentence ended\n",
    "            if next_word[0] == '.' or next_word[0] == '?' or next_word[0] == '!':\n",
    "                sentence_count += 1\n",
    "            \n",
    "            token_idx += 1\n",
    "\n",
    "        # format the text sequence into a sentence\n",
    "        text_string = \" \".join(text)\n",
    "        text_string = text_string.replace(\" . \",\". \").replace(\" , \",\", \").replace(\" ; \",\"; \").replace(\" ! \",\"! \").replace(\" ? \",\"? \").replace(\" : \",\": \")\n",
    "        text_string = text_string.replace(\"<TAB> \",\"\\t\").replace(\"<NL> \",\"\\n\")\n",
    "        text_string_final = text_string[:-2]+text_string[-1]\n",
    "        return text_string_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will not fail. Our country will thrive and prosper again. \n",
      "\n",
      "We assembled here today are issuing a new decree to be heard in every city near and far, small and large, from mountain to mountain, and from ocean to ocean, hear these words: \n",
      "\n",
      "You will never be ignored again.\n"
     ]
    }
   ],
   "source": [
    "with open('1789-04-30-first-inaugural-address.txt',encoding='utf-8') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "with open('2017-01-20-inaugural-address.txt',encoding='utf-8') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# with open('email.txt',encoding='utf-8') as f:\n",
    "#     contents = f.read()\n",
    "\n",
    "history_size = 3\n",
    "num_sentences = 3\n",
    "tl,ml = text_to_list(contents,history_size)\n",
    "d = gen_word_dist(tl,history_size,ml)\n",
    "print(generate_text(\"We will not\",num_sentences,d,history_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12fed321ce14db8c70d5fd7d87816765955c1efc0c914b111c5a8918a271e9da"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
