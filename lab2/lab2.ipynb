{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Logistic Regression & Linear Regression\n",
    "\n",
    "This lab will teach you how to build both a logistic regression and linear regression framework that you saw in class. Although these common algorithms can be utilized very quickly using pre-built functions provided by packages such as **scipy** and **pandas**, here we abstract the task by focusing on the algorithms used in these frameworks. \n",
    "\n",
    "To accomplish this, all the data has already been processed and provided to you so that the whole task can be done only using **numpy**. The only package that you need which was not used in the previous lab is **pickle**. Most python installations will come with pickle, so there is no need to create a new environment for this lab. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Overview\n",
    "### I. Data Processing\n",
    "\n",
    "### II. Logistic Regression (2 classes of MNIST digits)\n",
    "- Sigmoid\n",
    "- Negative Log Likelihood\n",
    "- Using Gradient Descent to solve Logistic Regression\n",
    "\n",
    "### III. Linear Regression (USA housing data)\n",
    "- Mean Squared Error\n",
    "- Using Gradient Descent to solve Linear Regression\n",
    "- Closed form solution of Linear Regression\n",
    "\n",
    "### You are responsible for finishing the functions provided to you in cells:\n",
    "- **Cell 11**: compute_loss()\n",
    "- **Cell 14**: GD()\n",
    "- **Cell 20**: compute_loss()\n",
    "- **Cell 22**: GD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Data Processing\n",
    "\n",
    "There are many ways to read and write data in python. One of the easiest (although not always efficient)\n",
    "ways to do this is to use [**pickle**](https://docs.python.org/3/library/pickle.html). I have already provided you with data that was pickled into a file (extension .p). Since the file should be located in the same location relative to this .ipynb jupyter lab, we can access it procedurally by first finding our current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GeffenPC\\Documents\\ECE194E\\ECE194E\\lab2\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "# display your current working directory\n",
    "print(cwd)\n",
    "# define the relative paths of the files we want\n",
    "mnist_path = '/data/mnist.p'\n",
    "housing_path = '/data/housing.p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your convenience, the data needed for this lab has been procesed into a [python dictionary](https://docs.python.org/3/tutorial/datastructures.html), where each value is a numpy array.  \n",
    "\n",
    "You can think of **dictionaries** as hash tables, or hash mappings, where each element (value) in the dictionary has a unique hash (key) associated with it. By nature, they are mutable data types (same as lists). In python, dictionary keys can be almost anything, or more specifically they have to be immutable objects (sets, integers, strings, even custom objects...) They are inducive to readable and efficient code due to the provided functionality of python and the versatility of hash maps respectively. Although they are not always the best data structure to use, some things are done much faster with dictinoaries. For example:\n",
    "\n",
    "Say I am recording some user data where my job is to keep track of the total score for every user. I have some list of user entires, where I have the username and the score for each user as an input. At any given point a new user can join by submitting their score. How would I efficiently keep track of all their scores given that I do not know the full set \n",
    "of users before hand?\n",
    "\n",
    " Dictionaries are perfect for this. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kyle': 6, 'neeli': 3, 'jen': 2, 'kyvaune': 3}\n",
      "{'kyle': 6, 'neeli': 3, 'jen': 2, 'kyvaune': 3}\n"
     ]
    }
   ],
   "source": [
    "# example data\n",
    "d = {}\n",
    "user_entries = [['kyle',5],['neeli',3],['jen',2],['kyvaune',3],['kyle',1]]\n",
    "# If else loop checking keys specifically\n",
    "for entry in user_entries:\n",
    "    if entry[0] in d.keys():\n",
    "        d[entry[0]] += entry[1]\n",
    "    else:\n",
    "        d[entry[0]] = entry[1]\n",
    "print(d)\n",
    "\n",
    "# Or using a try/except block\n",
    "d = {}\n",
    "user_entries = [['kyle',5],['neeli',3],['jen',2],['kyvaune',3],['kyle',1]]\n",
    "\n",
    "for entry in user_entries:\n",
    "    try:\n",
    "        d[entry[0]] += entry[1]\n",
    "    except:\n",
    "        d[entry[0]] = entry[1]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we load in the data using pickle's load function. Note 'rb' is used to declare that we are 'reading' the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['x_test', 'y_test', 'x_train', 'y_train'])\n",
      "x_test (1967, 28, 28)\n",
      "y_test (1967,)\n",
      "x_train (11867, 28, 28)\n",
      "y_train (11867,)\n"
     ]
    }
   ],
   "source": [
    "data = pickle.load(open(cwd+mnist_path,'rb'))\n",
    "# data is a dictionary\n",
    "print(data.keys())\n",
    "for key in data.keys():\n",
    "    print(key, data[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MNIST](http://yann.lecun.com/exdb/mnist/) is a well known dataset of handwritten digits containing 60,000 training images and 10,000 testing images. The dataset comes labeled, with a total of 10 classes (one for each digit.) For our task, I have preprocessed the data by taking all examples corresponding to the digits 6 and 9. My intuition is that these two digits should be easy to distinguish between..., can you guess why?\n",
    "\n",
    "The images in MNIST are 28 by 28 pixels. Hence the x_test and x_train datasets are numpy arrays of dimension (N, 28, 28) where N is the number of samples. Note how the testing set is smaller than the training set, this is common practice. \n",
    "\n",
    "The labels in MNIST are integers corresponding to the digits. Since we are working with binary logistic regression, we want our labels (or targets), to be zero or one. Hence I have re-labeled the datset for you so that **six corresponds to a label of zero and nine corresponds to a label of one.**\n",
    "\n",
    "x_test - Dataset of MNIST digit images (6 and 9) to be used for testing.\n",
    "\n",
    "y_test - Dataset of MNIST digit labels (0 and 1) to be used for testing.\n",
    "\n",
    "x_train - Dataset of MNIST digit labels (6 and 9) to be used for training.\n",
    "\n",
    "y_train - Dataset of MNIST digit labels (0 and 1) to be used for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [rcParams](https://matplotlib.org/stable/tutorials/introductory/customizing.html)\n",
    " provided by matplotlib makes it easier to control your plots. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import matplotlib as mpl\n",
    "# colors used\n",
    "ORANGE = '#FF9132'\n",
    "TEAL = '#0598B0'\n",
    "GREEN = '#008F00'\n",
    "PURPLE = '#8A2BE2'\n",
    "GRAY = '#969696'\n",
    "FIG_WIDTH = 4\n",
    "FIG_HEIGHT = 3\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"DejaVu Sans\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 15,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"lines.linewidth\": 2\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a function that will display our MNIST digits for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_mnist(img, labels):\n",
    "    # -1 is used to imply a dimension \n",
    "    img = img.reshape(-1,28)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    ticks = np.arange(14,img.shape[0],28)\n",
    "    # plt.yticks(ticks=ticks ,labels=labels)\n",
    "    plt.yticks(ticks ,labels)\n",
    "    plt.xticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lambda expressions](https://docs.python.org/3/tutorial/controlflow.html) can be useful in python when we want to define small expressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 9 9]\n"
     ]
    }
   ],
   "source": [
    "get_label = lambda x : 6 + 3*x\n",
    "print(get_label(np.array([0,1,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow_mnist(data['x_train'][0:4],get_label(data['y_train'][0:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Logistic Regression\n",
    "\n",
    "Logistic Regression is a classification algorithm used to differentiate data into discrete classes. The simplest form of this algorithm is the binary setting, where we have only two classes, $0$ and $1$. \n",
    "\n",
    "Given a dataset $\\mathcal{X}=\\{x_1,x_2,\\ldots,x_N\\}$ of $d$-dimensional vectors $x$, we have a corresponding set of labels $\\mathcal{Y}=\\{y_1,y_2,\\ldots,y_N\\}$ where  $y_i\\in\\{0,1\\}$. Our job is to build a classifier $h_{\\theta}$ that can calculate the probability a given $x$  belongs to class $1$, sometimes thought of as \"success\". We can think of this probability as a score, where our classifier outputs an estimate $0\\leq h_{\\theta}(x)\\leq1$. So what is $\\theta$ and what kind of function is our classifier $h_{\\theta}$?\n",
    "\n",
    "$$h_{\\theta}=\\sigma(\\theta^\\intercal x).$$\n",
    "\n",
    "We can see that $\\theta$ is a $d$-dimensional vector that maps our input vector $x$ to a real number. This is done in linear regression to map our data to a continous value, but since we want a probability of belonging to a given class we can not stop here. This is where the sigmoid function comes in, we use it to map this value between zero and one. \n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Why would such a function be used here, let's take a look at it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the x_min, x_max values below and see what it looks like.\n",
    "\n",
    "\n",
    "We can see that it is quite sensative to the range, with its behaviour going from a step function to a line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_min, x_max = -10, 10\n",
    "# x_min, x_max = -1,1\n",
    "# x_min, x_max = -1000,1000\n",
    "\n",
    "x = np.arange(x_min,x_max,0.01)\n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(FIG_WIDTH,FIG_HEIGHT))\n",
    "axs.plot(x,sigmoid(x),color=PURPLE)\n",
    "plt.title('Sigmoid(x)')\n",
    "axs.set_xlabel('x')\n",
    "axs.set_xlim(x_min,x_max)\n",
    "axs.set_ylim(0,1)\n",
    "axs.set_yticks([0,0.5,1])\n",
    "axs.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function shown above outputs a probability value between $0$ and $1$ given a real value as an input. In the scope of binary logistic regression, our input is some data sample $x\\in\\mathcal{X}$, which has a corresponding true label $y$. We can think of the overall classifier as such:\n",
    "$$\n",
    "\\begin{align} \n",
    "h_{\\theta}(x) &= P(y=1|x,\\theta),\\\\\n",
    "1-h_{\\theta}(x) &= P(y=0|x,\\theta),\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "where we predict that our data sample $x$ is in class $y=1$ if $h_{\\theta}(x)\\geq 0.5$.\n",
    "\n",
    "How would we train such a classifier, or better yet, how do we choose $\\theta$ so that our classifier is as best as it can be? If we knew how $x$ was distributed based on the label, we can solve this problem exactly using Baye's rule and what we  have learned about the Maximum Likelihood Estimate. But in this case, we are not given any information on how $x$ is distributed, and hence must learn the right $\\theta$ some other way. To do this, we must first build a metric that desides how good a given choice of $\\theta$ is at classifying our data. A general metric is shown below:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N}cost(h_{\\theta}(x_i),y_i)\n",
    "$$\n",
    "\n",
    "So given a datset $\\mathcal{X}$, we can sum over the individual data points $x_i$, checking how close our predicted label $h_{\\theta}(x_i)$ is to the true target $y_i$ for each data point. You can see that this depends on not only the chosen value of $\\theta$, but also the dataset $\\mathcal{X}$. What should the cost be in this expression? Since we are working with probability values we use the log likelihood loss function, where:\n",
    "$$\n",
    "cost(h_{\\theta}(x),y)= \\begin{cases} -log(h_{\\theta}(x)) \\ &\\text{if} \\ y=1 \\\\ -log(1-h_{\\theta}(x)) \\ &\\text{if} \\ y=0  \\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "The intuition here is that the log function discounts greatly if we are far away from the true label, and less so when we are close. But this is not so convenient for us, as we do not want to deal with this conditional switching in our overall loss function $J(\\theta)$. Luckily, a simpler cost function can be defined, that still has the same value as the expression above:\n",
    "$$\n",
    "cost(h_{\\theta}(x),y)= -y log(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x))\n",
    "$$\n",
    "\n",
    "Your first task is to construct the defined loss function, $J(\\theta)$, in the cell below. Remember that $J$ acts on a given dataset $\\mathcal{X}$. Sometimes it is easier to think of our dataset as an $N$ by $d$ matrix, since this is how we work with it in python/numpy. \n",
    "\n",
    "$$\n",
    "X=\n",
    "\\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\\n",
    "\\vdots\\\\\n",
    "x_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Remember that $N$ is the size of the dataset, and $d$ is the dimension of each data sample $x$. Since MNIST is a dataset of images, we want to flatten the image vectors before passing them along to our loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x, y, theta):\n",
    "    '''\n",
    "    Computes the Negative Log-Likelihood Loss\n",
    "    Inputs:\n",
    "        x     - np.array of MNIST digits size (N,28,28) or (28,28)\n",
    "        y     - np.array of labels 0 for 'six' and 1 for 'nine'\n",
    "        theta - np.array of the parameter vector we want to learn\n",
    "        \n",
    "    HINT: The computer does not like log(0)\n",
    "    '''\n",
    "    \n",
    "    # reshape the dataset to be (N,d) where N = number of samples, d = 28*28\n",
    "    x = x.reshape(-1,28*28)\n",
    "    N = len(x)\n",
    "\n",
    "    # define some epsilon value to avoid log(0), ideally sigmoid should never\n",
    "    #  return 0 exactly but we have finite precision\n",
    "    eps = np.finfo(float).eps\n",
    "\n",
    "    # # model output for every sample --> (N,1)\n",
    "    h_theta = sigmoid(np.dot(x,theta))\n",
    "\n",
    "    # total cost over all samples, y --> (N,1) do inner product for multiplication to get scalar loss\n",
    "    cost = -np.dot(y,np.log(h_theta+eps)) - np.dot((1-y),np.log((1-h_theta)+eps))\n",
    "    \n",
    "    # average over all samples\n",
    "    return cost/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the above function, lets check that it works both on individual data samples from our MNIST dataset, and for subsets of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.23725587e-05]\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(seed=123)\n",
    "theta = rng.random(28*28)\n",
    "x = data['x_train'][0].flatten()\n",
    "y = data['y_train'][0]\n",
    "print(compute_loss(x,y,theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.186200900463472\n"
     ]
    }
   ],
   "source": [
    "x = data['x_train']\n",
    "y = data['y_train']\n",
    "print(compute_loss(x,y,theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our fancy loss function $J(\\theta)$, how do we now use it to choose $\\theta^*$, the best $\\theta$. We want a $\\theta^*$ that minimizes our loss function when evaluated over the entire dataset, which in our case is the training set provided. \n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\min_{\\theta} J(\\theta)\n",
    "$$\n",
    "\n",
    "Okay so lets just use what we know from calculus, we take the derivative of $J$ with respsect to $\\theta$, set that to zero and then... oh wait. If you go about this route, you will see that unless we had a closed form differentiable expression for $P(y|x)$, we can not solve the problem this way. Instead we rely on a famous numerical technique known as Gradient Descent (GD). \n",
    "\n",
    "GD assumes the given function is convex (you can think of that as meaning it only has one global minimum), and goes about finding the minimum value of the function. Essentially you can think of it as taking steps in the direction of the negative derivative (downhill), until it reaches the bottom of our function. So at every iteration of GD, we can update our current estimate of the paramater $\\theta$, by taking a step in the direction of the negative derivate (gradient) of $J(\\theta)$:\n",
    "\n",
    "$$\n",
    "\\theta : \\theta -\\alpha \\frac{\\delta}{\\delta\\theta}\\Big[J(\\theta)\\Big],\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the size of the steps we take. Intuitively, GD asks \"which elements of $\\theta$ can I tune so that our loss evaluated over the entire dataset decreases?\". A larger gradient in those elements of $\\theta$ corresponds to the loss being more sensetive in those dimensions. As we continue to iterate and update $\\theta$ in this fashion, we are gauarnteed to reach the global minimum given that our function $J$ is convex. Even though our function may be highly non-convex, this approach is computationally efficient, and hence is used widely even in more complex settings such as Neural Networks, where there are no such gaurantees. \n",
    "\n",
    "For logistic regression, the gradient of $J$ is computed for you and shown below. For linear regression in the next part, it will be your job. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta J}{\\delta \\theta} &= \\frac{\\delta}{\\delta \\theta}\\Big(\\frac{1}{N}\\sum_{i=1}^{N}cost(h_{\\theta}(x_i),y_i)\\Big),\\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\delta}{\\delta \\theta}cost(h_{\\theta}(x_i),y_i),\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta \\ cost(h_{\\theta}(x_i),y_i)}{\\delta \\theta} &= \\frac{\\delta}{\\delta \\theta}\\Big(-y log(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x))\\Big).\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Given that $\\frac{\\delta}{\\delta x}\\sigma(x) = \\sigma(x)(1-\\sigma(x))$ one can show:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta J}{\\delta \\theta[j]} &= \\frac{1}{N}\\sum_{i=1}^{N}\\big((h_{\\theta}(x_i)-y_i)x_i[j]\\big)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note the indexing over the elements of $\\theta$ which is needed when computing the gradient vector. The $j$ here denotes the dimension of $\\theta$ at which we are evaluating this partial derivative, and hence we use the $j$'th element of datasamples $x_i$ to weight our derivative. \n",
    "\n",
    "Your task is to implement a Graident Descent algorithm that utilizes this gradient to make updates to a given $\\theta$ for some pre defined number of iterations. Most of the code is provided to you, so your task is to finsh the given function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(x, y, theta, lr, num_iters):\n",
    "    '''\n",
    "    Computes gradient descent to minimize the NLL\n",
    "    Inputs:\n",
    "        x         - np.array of MNIST images\n",
    "        y         - np.array representing the true labels\n",
    "        theta     - np.array of the parameter vector we want to learn (initial guess)\n",
    "        lr        - learning rate or step size \n",
    "        num_iters - number of steps we take\n",
    "    '''\n",
    "    # we will want x to be in the right format\n",
    "    x = x.reshape(-1,28*28)\n",
    "    N = len(x)\n",
    "    # we may also want to store the computed loss as we iterate\n",
    "    loss = np.zeros(num_iters)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # compute the loss for this iteration ()total loss so average it)\n",
    "        loss[i] = compute_loss(x,y,theta)\n",
    "\n",
    "        # calculate the output (used by gradient)\n",
    "        h_theta = sigmoid(np.dot(x,theta))\n",
    "\n",
    "        # calculate the gradient (average over all samples)\n",
    "        dj_dtheta = (1/N)*np.dot((h_theta-y),x)\n",
    "\n",
    "        # update the parameters\n",
    "        theta = theta - lr*dj_dtheta\n",
    "\n",
    "    return loss, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our learned value of $\\theta^*$, we want to make some predictions on the unseen data in our test dataset. This is commonly used as a metric for classification problems, since testing on the same dataset you train on can give biased results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, theta):\n",
    "    return np.round(sigmoid(np.dot(x.reshape(-1,28*28),theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can test your gradient descent implementation. Is your loss improving (decreasing) as you iterate? Note the chosen value for learning rate and number of iteration can be changed, although it should work as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost is: 14.889\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = data['x_train']\n",
    "y = data['y_train']\n",
    "\n",
    "num_iters = 100\n",
    "lr = 0.1\n",
    "theta = rng.random(28*28)\n",
    "print(\"Initial Cost is: %.3f\"%compute_loss(x, y, theta))\n",
    "\n",
    "loss, theta_star = GD(x, y, theta, lr, num_iters)\n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(FIG_WIDTH,FIG_HEIGHT))\n",
    "axs.plot(range(len(loss)), loss, 'ORANGE')\n",
    "plt.title(\"$J(\\\\theta)$\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above plot of our loss fucntion looks like it has converged, we can see how our accuracy is on the test dataset. The solution should result in an accuracy of above 95%. We can also look at the some of the misclassified images to get a sense of what caused the classification errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 96.950\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = data['x_test']\n",
    "y = data['y_test']\n",
    "\n",
    "y_predict = predict(x, theta_star)\n",
    "print(\"Accuracy on test set: %.3f\"%(np.mean(y_predict==y)*100))\n",
    "imshow_mnist(x[y_predict!=y][0:10],get_label(y[y_predict!=y][0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Linear Regression (USA housing data)\n",
    "\n",
    "Now that we have looked at logistic regression, I will let you implement a multi-class linear regression framework. A lot of the theory is the same here, but the main difference is in the cost function used to evaluate how close the predicted values are to the true ones. Remember that in linear regression, we are trying to estimate a continous value, and hence we do not need to use a sigmoid in order to map our values to probabilities. Instead, we rely on the Mean Squared Error as our metric of cost, and sum this error over the whole dataset given a $\\theta$.\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2N}\\sum_{i=1}^{N}(\\theta^\\intercal x_i -y_i)^2,\n",
    "$$\n",
    "\n",
    "where the inner summation is the Mean Squared Error between the true value $y_i$, and our prediction $\\theta^\\intercal x_i$. \n",
    "\n",
    "Just like in the previous task, your job is to construct a linear regression framework using gradient descent by computing the gradient of the above loss function. The dataset we will work with is the USA housing dataset. It has been converted for you in the same manner, split into training and testing datasets of labels and features.  \n",
    "\n",
    "Each data sample $x$ is a row of the full data set $X$, where the 'features' are the columns of $X$. In this dataset, there are five features:\n",
    "\n",
    " 0     Avg. Area Income              \n",
    "\n",
    " 1     Avg. Area House Age           \n",
    "\n",
    " 2     Avg. Area Number of Rooms     \n",
    "\n",
    " 3     Avg. Area Number of Bedrooms  \n",
    "\n",
    " 4     Area Population \n",
    "\n",
    "The provided labels give the corresponding house's price for each data sample $x$. Your job is to create a linear function $\\theta^\\intercal x$, that can estimate house prices. This is the standard linear regression setting.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['x_train', 'y_train', 'x_test', 'y_test'])\n",
      "x_train (4500, 5)\n",
      "y_train (4500,)\n",
      "x_test (500, 5)\n",
      "y_test (500,)\n"
     ]
    }
   ],
   "source": [
    "data = pickle.load(open(cwd+housing_path,'rb'))\n",
    "print(data.keys())\n",
    "for key in data.keys():\n",
    "    print(key, data[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features vary greatly in their distribution, since they are not just pixel values. The number of bedrooms is a good dictator of home price, but its magnitude is small compared to the average income in the neighberhood. Because of this, we first normalize the individual features, so that they have the same weight. Also, since we want to account for a bias term (think of zero-intercept in 2-dimensions,) we can do this directly by injecting $1$ as the first feature of every data point $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    '''\n",
    "    Normalizes the input data and add ones column for bias\n",
    "    '''\n",
    "    new_x = np.ones((len(x),6))\n",
    "    for col in range(x.shape[1]):\n",
    "        new_x[:,col+1] = (x[:,col]-x[:,col].mean())/x[:,col].std()\n",
    "\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is as before, finish the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x, y, theta):\n",
    "    '''\n",
    "    Computes the MSE or L2 loss\n",
    "    Inputs:\n",
    "        x     - np.array of arbitrary length and width of 6\n",
    "        y     - np.array representing true housing price\n",
    "        theta - np.array of the parameter vector we want to learn\n",
    "    '''\n",
    "    \n",
    "    # get the dataset length\n",
    "    N = x.shape[0]\n",
    "\n",
    "    # compute the loss over all samples\n",
    "    loss = (1/(2*N))*np.sum(np.square(np.dot(x,theta)-y))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does your function work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824456720792.9413\n",
      "907995.9916172215\n"
     ]
    }
   ],
   "source": [
    "x = normalize(data['x_train'])\n",
    "y = data['y_train']\n",
    "\n",
    "theta = rng.random(6)\n",
    "print(compute_loss(x,y,theta))\n",
    "\n",
    "# square root gives more intuitive value ~1,000,000\n",
    "print(np.sqrt(compute_loss(x,y,theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same as before, constructing the gradient descent algorithm for linear regression by taking steps in the direction of the negative gradient of $J(\\theta)$ with respect to theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the gradient:\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2N}\\sum_{i=1}^{N}(\\theta^\\intercal x_i -y_i)^2,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta}\\Big(\\frac{1}{2N}\\sum_{i=1}^{N}(\\theta^\\intercal x_i -y_i)^2\\Big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{N}\\Big(\\sum_{i=1}^{N}(\\theta^\\intercal x_i -y_i)\\Big)x_i[j]\n",
    "$$\n",
    "\n",
    "Overall the gradient then can be expressed in vector notation:\n",
    "$$\n",
    "\\nabla J(\\theta) = \\frac{1}{N}(X\\theta-y)X\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(x, y, theta, lr, num_iters):\n",
    "    '''\n",
    "    Computes gradient descent to minimize the MSE\n",
    "    Inputs:\n",
    "        x         - np.array of arbitrary length and width of 5\n",
    "        y         - np.array representing true housing price\n",
    "        theta     - np.array of the parameter vector we want to learn\n",
    "        lr        - learning rate or step size \n",
    "        num_iters - number of steps we take\n",
    "\n",
    "    HINT: Dont forget that we normalize(x) before passing it into this function,    \n",
    "          which adds an extra dimension to our data.\n",
    "    '''\n",
    "    # your code goes here\n",
    "    cost = np.zeros(num_iters)\n",
    "    N = len(x)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # get the current cost\n",
    "        cost[i] = compute_loss(x,y,theta)\n",
    "\n",
    "        # find the gradient\n",
    "        grad = (1/N)*np.dot((np.dot(x,theta)-y),x)\n",
    "\n",
    "        # update the parameters\n",
    "        theta = theta -lr*grad\n",
    "    return cost, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, theta):\n",
    "    return np.dot(x.reshape(-1,6),theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how your algorithm preforms. I have chosen arbitrary values for num_iters and learning rate, play around with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost is: 824456803768.559\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = normalize(data['x_train'])\n",
    "y = data['y_train']\n",
    "\n",
    "#play around with these values\n",
    "num_iters = 100 # set this to converge to ideal loss using closed form\n",
    "lr = 0.25\n",
    "theta = rng.random(6)\n",
    "print(\"Initial Cost is: %.3f\"%compute_loss(x, y, theta))\n",
    "\n",
    "cost, theta = GD(x, y, theta, lr, num_iters)\n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(FIG_WIDTH,FIG_HEIGHT))\n",
    "axs.plot(range(len(cost)), cost, 'ORANGE')\n",
    "plt.title(\"$J(\\\\theta)$\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike logistic regression, linear regression has a closed form solution (when we set the gradient to zero) given a data matrix $X$. In 2-dimensions you may recall this closed form solution as finding the best-fit line for a dataset. The math in d-dimensions is almost identical but relies on taking the L2 norm between the predictions of the dataset $X\\theta$ (this is a column vector of dimension $N$), and all of our labels $y$ (also a column vector).\n",
    "\n",
    "$$\n",
    "\\theta^* = (X^\\intercal X)^{-1}X^\\intercal \\mathbf{y},\n",
    "$$\n",
    "where boldfaced $\\mathbf{y}$ is used to denote that we are referring to the whole dataset, and hence we need the full vector of labels. \n",
    "\n",
    "The closed form solution for $\\theta^*$ is provided below as \"true_theta\". We can compare our loss to this solution to see how close we are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after running lin reg with gradient descent: 5086664120.750\n",
      "Loss using ideal parameters: 5086664120.749\n"
     ]
    }
   ],
   "source": [
    "temp = np.matmul(x.T,x)\n",
    "inv = np.linalg.inv(temp)\n",
    "true_theta = np.dot(np.matmul(inv,x.T),y)\n",
    "\n",
    "print(\"Loss after running lin reg with gradient descent: %.3f\"%compute_loss(x,y,theta))\n",
    "print(\"Loss using ideal parameters: %.3f\"%compute_loss(x,y,true_theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If these numbers are close, we can move on to see how good our estimator is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average house price: 1214045.125\n",
      "Std. of house prices: 335494.469\n",
      "Average abs. error of our prediction: 83465.195\n"
     ]
    }
   ],
   "source": [
    "x = normalize(data['x_test'])\n",
    "y = data['y_test']\n",
    "\n",
    "y_predict = predict(x, theta)\n",
    "# find average error\n",
    "np.mean(abs(y_predict-y))\n",
    "print('Average house price: %.3f'%y.mean())\n",
    "print('Std. of house prices: %.3f'%y.std())\n",
    "print('Average abs. error of our prediction: %.3f'%np.mean(abs(y_predict-y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the average absolute error over the testset is an order of magnitude smaller than the standard deviation of the testset. This means we can make accurate predictions, just by applying a very simple algorithm on our dataset. This of course can be improved on by utilizing more features, constructing different kernels for our features, and using an L-2 penalty as shown in class for ridge-regression. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('lab1': conda)",
   "name": "python385jvsc74a57bd0e14277392997ec7b8d0d4e1eacc6a12462d831e78f154a1c368aae24d965e9e8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "e14277392997ec7b8d0d4e1eacc6a12462d831e78f154a1c368aae24d965e9e8"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
